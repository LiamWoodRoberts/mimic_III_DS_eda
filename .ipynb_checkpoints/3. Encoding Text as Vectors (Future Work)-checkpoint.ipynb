{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from section_parse import run\n",
    "from nltk import RegexpChunkParser,word_tokenize\n",
    "from nltk.chunk.regexp import ChunkRule,ChinkRule,SplitRule,MergeRule\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Discharge Summary Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"HISTORY OF PRESENT ILLNESS:\"\n",
    "#title = \"DISCHARGE MEDICATIONS:\"\n",
    "medication_sections = run(title)\n",
    "medication_sections = [i for i in medication_sections if i != \"NOT FOUND\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    bad_chars = [\":\",\"*\"]\n",
    "    space_chars = [\"[\",\"]\",\"(\",\")\"]\n",
    "    for c in bad_chars:\n",
    "        text = text.replace(c,\"\")\n",
    "    for c in space_chars:\n",
    "        text = text.replace(c,\" \")\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history of present illness\n",
      " known firstname    known lastname 1852  is a 62-year-old left-handed man who is here for a\n",
      "follow up of his left sphenoid meningioma.  i last saw him on\n",
      " 2149-11-17  and his head ct showed growth of the left sphenoid\n",
      "meningioma.  he is seizure free.  today, he is here with his\n",
      "wife\n",
      "and daughter.   name  ni    does not have headache, nausea, vomiting,\n",
      "urinary incontinence, or fall.\n",
      "\n",
      "his neurological problem began on  2142-6-22  when he became\n",
      "confused and disoriented in a hotel bathroom.  at that time, he\n",
      "was visiting his daughter for a wedding.  his wife found him\n",
      "slumped over in the bath tube.  according to her, his eyes\n",
      "looked\n",
      "funny.  he could not stand up.  his verbal output did not make\n",
      "sense.  he was brought to  doctor first name 1853  hospital in placentia,\n",
      "ca.  he woke up 7 to 8 hours later in the emergency room.  he\n",
      "felt very tired after the event.  he was hospitalized from\n",
      " 2142-6-22  to  2142-6-25 .  he had a cardiac pacemaker placement due\n",
      "to irregular heart rate and bradycardia.  he also had a head mri\n",
      "that showed a less than 1 cm diameter sphenoid meningioma.\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(medication_sections[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Entity Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Entity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>pyridostigmine bromide syrup</td>\n",
       "      <td>DRUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>critic-aid clear af</td>\n",
       "      <td>DRUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ibup</td>\n",
       "      <td>DRUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>posaconazole oral liquid (*ind*)</td>\n",
       "      <td>DRUG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>byetta</td>\n",
       "      <td>DRUG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Name Entity\n",
       "0      pyridostigmine bromide syrup   DRUG\n",
       "1               critic-aid clear af   DRUG\n",
       "2                              ibup   DRUG\n",
       "3  posaconazole oral liquid (*ind*)   DRUG\n",
       "4                            byetta   DRUG"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_df = pd.read_csv(\"./entities.csv\")\n",
    "bad_ents = [\"solution\",\"dose\",\"lot\",\"enema\",\"-\",\"in\",\"can\",\"pack\",\"ring\",\"bar\",\"bags\",\"cart\",\"jar\",\"pad\",\"as\",\"it\",\"in\"]\n",
    "ent_df = ent_df[ent_df[\"Name\"].isin(bad_ents)==0].copy()\n",
    "section_mask = ent_df[\"Entity\"].isin([\"DRUG\",\"ROUTE\",\"UNIT\",\"CONDITION\",\"SYMPTOM\",\"DOSE\"])\n",
    "ent_df = ent_df[section_mask].dropna()\n",
    "ent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting Data for ML Models\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Convert list of sections to list of sentances\n",
    "2. Tokenize sentances and get POS for each word\n",
    "3. Parse certain tokens into chunks\n",
    "4. Get medical entity tag for token / chunks from entity data frame\n",
    "5. Output as list of sequences of words,POS,and labels.\n",
    "6. Format sequences by padding to equal lengths for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(ent_name):\n",
    "    return ent_df[ent_df[\"Name\"]==ent_name]\n",
    "\n",
    "def sections_to_sentances(sections):\n",
    "    sentances = []\n",
    "    for section in sections:\n",
    "        section = clean_text(section)\n",
    "        section_sentances = section.split(\".\")\n",
    "        sentances += [i for i in section_sentances if len(i)>0]\n",
    "    return sentances\n",
    "\n",
    "def parse_pos(text):\n",
    "    text = word_tokenize(text)\n",
    "    pos = nltk.pos_tag(text)\n",
    "    return pos\n",
    "\n",
    "def parse_chunks(pos):\n",
    "    cr1 = ChunkRule(\"<NN><IN><NN>+\",\"Chunk Some Stuff\")\n",
    "    cr2 = ChunkRule(\"<NN><NN>\",\"chunk noun pairs\")\n",
    "    cr3 = ChunkRule(\"<NN><NNS>\",\"chunk noun and nns pairs\")\n",
    "    cr4 = ChunkRule(\"<JJ><NNS>\",\"chunk other stuff\")\n",
    "    cr5 = ChunkRule(\"<JJ>\"\"<JJ>\",\"yet more chunks\")\n",
    "    chunk_parser = RegexpChunkParser([cr1,cr2,cr3,cr4,cr5],chunk_label=\"NP\")\n",
    "    chunked_text = chunk_parser.parse(pos)\n",
    "    return chunked_text\n",
    "\n",
    "def format_chunks(chunks):\n",
    "    formatted_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if type(chunk) != tuple:\n",
    "            chunk = (' '.join([i[0] for i in chunk.leaves()]),'NP')\n",
    "        formatted_chunks.append(chunk)\n",
    "    return formatted_chunks\n",
    "    \n",
    "def return_chunk_ent_type(name,ent_df):\n",
    "    mask = ent_df[\"Name\"] == name\n",
    "    if sum(mask) > 0:\n",
    "        return ent_df[mask][\"Entity\"].iloc[0]\n",
    "    \n",
    "    elif len(name.split())>1:\n",
    "        for word in name.split():\n",
    "            mask = ent_df[\"Name\"]==word\n",
    "            if sum(mask) > 0:\n",
    "                return ent_df[mask][\"Entity\"].iloc[0]\n",
    "    return 'O'\n",
    "    \n",
    "def medical_chunker(text,ent_df=ent_df):\n",
    "    pos = parse_pos(text)\n",
    "    chunked_text = parse_chunks(pos)\n",
    "    chunks = format_chunks(chunked_text)\n",
    "    chunk_df = pd.DataFrame(data=chunks,columns=[\"Name\",'POS'])\n",
    "    \n",
    "    # Tag single words\n",
    "    chunk_df[\"TAG\"] = chunk_df[\"Name\"].apply(lambda x:return_chunk_ent_type(x,ent_df))\n",
    "    return chunk_df.values\n",
    "\n",
    "def create_dataset(sections,ent_df):\n",
    "    \n",
    "    sentances = sections_to_sentances(sections)\n",
    "    dataset = []\n",
    "    \n",
    "    # create a df for each sentance and combine\n",
    "    for i,sentance in enumerate(sentances):\n",
    "        sequence = medical_chunker(sentance,ent_df=ent_df)\n",
    "        dataset.append(sequence)\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    }
   ],
   "source": [
    "sections = medication_sections[:100]\n",
    "seqs = create_dataset(sections,ent_df)\n",
    "seqs = [i for i in seqs if len(i)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting Words -> Vectors\n",
    "\n",
    "Words are now formatted as sequences for each sentance with corresponding POS and entity tags. Words are now converted to number values / ids and padded to the appropriate length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sequence length: 98 \n",
      "\n",
      "Sample:\n",
      "[['history' 'NN' 'O']\n",
      " ['of' 'IN' 'O']\n",
      " ['present' 'JJ' 'O']\n",
      " ['illness' 'NN' 'O']\n",
      " ['this' 'DT' 'O']\n",
      " ['is' 'VBZ' 'O']\n",
      " ['an' 'DT' 'O']\n",
      " ['81-year-old' 'JJ' 'O']\n",
      " ['female' 'NN' 'O']\n",
      " ['with' 'IN' 'O']\n",
      " ['a' 'DT' 'O']\n",
      " ['history of emphysema' 'NP' 'CONDITION']\n",
      " ['not' 'RB' 'O']\n",
      " ['on' 'IN' 'O']\n",
      " ['home o2' 'NP' 'UNIT']\n",
      " [',' ',' 'O']\n",
      " ['who' 'WP' 'O']\n",
      " ['presents' 'VBZ' 'O']\n",
      " ['with' 'IN' 'O']\n",
      " ['three' 'CD' 'O']\n",
      " ['days' 'NNS' 'O']\n",
      " ['of' 'IN' 'O']\n",
      " ['shortness of breath' 'NP' 'SYMPTOM']\n",
      " ['thought' 'VBN' 'O']\n",
      " ['by' 'IN' 'O']\n",
      " ['her' 'PRP$' 'O']\n",
      " ['primary' 'JJ' 'O']\n",
      " ['care doctor' 'NP' 'O']\n",
      " ['to' 'TO' 'O']\n",
      " ['be' 'VB' 'O']\n",
      " ['a' 'DT' 'O']\n",
      " ['copd flare' 'NP' 'CONDITION']]\n"
     ]
    }
   ],
   "source": [
    "print(\"max sequence length:\",max([len(i) for i in seqs]),\"\\n\")\n",
    "print(\"Sample:\")\n",
    "print(seqs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_ids(sentances,feature = 0):\n",
    "    words = []\n",
    "    for sentance in sentances:\n",
    "        words += list([word[feature] for word in sentance])\n",
    "    word_dict = {word:i for i,word in enumerate(set(words))}\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = get_word_ids(seqs,0)\n",
    "pos_ids = get_word_ids(seqs,1)\n",
    "tag_ids = get_word_ids(seqs,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_ids(sentances,word_ids,tag_ids,pos_ids):\n",
    "    vector = []\n",
    "    for sentance in sentances:\n",
    "        vector.append(list([[word_ids[w[0]],pos_ids[w[1]],tag_ids[w[2]]] for w in sentance]))\n",
    "    return np.array(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric Representation:\n",
      "[[515, 8, 2], [740, 3, 2], [1836, 14, 2], [2035, 8, 2]]\n",
      "\n",
      "Word Representation:\n",
      "[['history' 'NN' 'O']\n",
      " ['of' 'IN' 'O']\n",
      " ['present' 'JJ' 'O']\n",
      " ['illness' 'NN' 'O']]\n"
     ]
    }
   ],
   "source": [
    "vectors = words_to_ids(seqs,word_ids,tag_ids,pos_ids)\n",
    "print(\"Numeric Representation:\")\n",
    "print(vectors[0][:4])\n",
    "print('')\n",
    "print(\"Word Representation:\")\n",
    "print(seqs[0][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding Sequences and Creating X,Y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(vectors,length):\n",
    "    matrix = []\n",
    "    for v in vectors:\n",
    "        pad_length = length-len(v)\n",
    "        fill = np.zeros([pad_length,3])\n",
    "        padded_seq = np.vstack([v,fill])\n",
    "        matrix.append(padded_seq)\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1640, 98, 3)\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(i) for i in vectors])\n",
    "matrix = pad_sequences(vectors,max_length)\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.150e+02 8.000e+00 2.000e+00]\n",
      " [7.400e+02 3.000e+00 2.000e+00]\n",
      " [1.836e+03 1.400e+01 2.000e+00]\n",
      " [2.035e+03 8.000e+00 2.000e+00]\n",
      " [1.615e+03 2.800e+01 2.000e+00]] \n",
      "...\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(matrix[0][:5],\"\\n...\\n\",matrix[0][-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_y(matrix):\n",
    "    x = []\n",
    "    y = []\n",
    "    for sequences in matrix:\n",
    "        xi = [[i[0],i[1]] for i in sequences]\n",
    "        yi = [[i[2] for i in sequences]]\n",
    "        x.append(xi)\n",
    "        y.append(yi)\n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-shape: (1640, 98, 2)\n",
      "[[ 515.    8.]\n",
      " [ 740.    3.]\n",
      " [1836.   14.]\n",
      " [2035.    8.]\n",
      " [1615.   28.]]\n",
      "y-shape: (1640, 98)\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "x,y = create_x_y(matrix)\n",
    "y = np.reshape(y,[-1,y.shape[-1]])\n",
    "print(\"x-shape:\",x.shape)\n",
    "print(x[0][:5])\n",
    "print(\"y-shape:\",y.shape)\n",
    "print(y[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras36]",
   "language": "python",
   "name": "conda-env-keras36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
